wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: tenlhak98 (tenlhak98-university-at-buffalo) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.7
wandb: Run data is saved locally in /home/dapgrad/tenzinl2/lumina/tabel_extractor/wandb/run-20250311_165524-89jonsl8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tablesense-20250311-165524
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tenlhak98-university-at-buffalo/tablesense
wandb: üöÄ View run at https://wandb.ai/tenlhak98-university-at-buffalo/tablesense/runs/89jonsl8
2025-03-11 16:55:26,296 - INFO - Initialized wandb run: tablesense-20250311-165524
2025-03-11 16:55:26,297 - INFO - Process 0/8 starting training
[rank6]:[E311 18:55:28.484471411 ProcessGroupNCCL.cpp:629] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200017 milliseconds before timing out.
[rank6]:[E311 18:55:28.493777518 ProcessGroupNCCL.cpp:1936] Rank 6
	 - [6] Timeout at collective: ALLGATHER, #1
	 - To our best knowledge, the lagging/dead/mismatched ranks that caused the desync are:
	   - [0, 1, 2, 3, 4, 5, 6, 7] joined but didn't finish collective #1 (count from 1)
	 - Snapshot of ranks' latest states:
	   #1 started ranks:
	     [0, 1, 2, 3, 4, 5, 6, 7] started ALLGATHER
[rank6]:[E311 18:55:28.493850505 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0 Rank 6]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank4]:[E311 18:55:28.521225703 ProcessGroupNCCL.cpp:629] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200040 milliseconds before timing out.
[rank4]:[E311 18:55:28.522469408 ProcessGroupNCCL.cpp:1936] Rank 4
	 - [4] Timeout at collective: ALLGATHER, #1
	 - To our best knowledge, the lagging/dead/mismatched ranks that caused the desync are:
	   - [0, 1, 2, 3, 4, 5, 6, 7] joined but didn't finish collective #1 (count from 1)
	 - Snapshot of ranks' latest states:
	   #1 started ranks:
	     [0, 1, 2, 3, 4, 5, 6, 7] started ALLGATHER
[rank4]:[E311 18:55:28.522545655 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0 Rank 4]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank0]:[E311 18:55:28.531921574 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200028 milliseconds before timing out.
[rank0]:[E311 18:55:28.533107007 ProcessGroupNCCL.cpp:1936] Rank 0
	 - [0] Timeout at collective: ALLGATHER, #1
	 - To our best knowledge, the lagging/dead/mismatched ranks that caused the desync are:
	   - [0, 1, 2, 3, 4, 5, 6, 7] joined but didn't finish collective #1 (count from 1)
	 - Snapshot of ranks' latest states:
	   #1 started ranks:
	     [0, 1, 2, 3, 4, 5, 6, 7] started ALLGATHER
[rank0]:[E311 18:55:28.533120322 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0 Rank 0]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank2]:[E311 18:55:28.538686478 ProcessGroupNCCL.cpp:629] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200066 milliseconds before timing out.
[rank6]:[E311 18:55:28.539142929 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 _verify_param_shape_across_processes from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/distributed/utils.py:294
#1 __init__ from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:825
#2 train_model_distributed from /home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:579
#3 _wrap from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:90
#4 run from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:108
#5 _bootstrap from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:314
#6 _main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:135
#7 spawn_main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:122
#8 <module> from <string>:1

[rank7]:[E311 18:55:28.539667806 ProcessGroupNCCL.cpp:629] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200054 milliseconds before timing out.
[rank2]:[E311 18:55:28.540128465 ProcessGroupNCCL.cpp:1936] Rank 2
	 - [2] Timeout at collective: ALLGATHER, #1
	 - To our best knowledge, the lagging/dead/mismatched ranks that caused the desync are:
	   - [0, 1, 2, 3, 4, 5, 6, 7] joined but didn't finish collective #1 (count from 1)
	 - Snapshot of ranks' latest states:
	   #1 started ranks:
	     [0, 1, 2, 3, 4, 5, 6, 7] started ALLGATHER
[rank2]:[E311 18:55:28.540188060 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0 Rank 2]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank7]:[E311 18:55:28.541084460 ProcessGroupNCCL.cpp:1936] Rank 7
	 - [7] Timeout at collective: ALLGATHER, #1
	 - To our best knowledge, the lagging/dead/mismatched ranks that caused the desync are:
	   - [0, 1, 2, 3, 4, 5, 6, 7] joined but didn't finish collective #1 (count from 1)
	 - Snapshot of ranks' latest states:
	   #1 started ranks:
	     [0, 1, 2, 3, 4, 5, 6, 7] started ALLGATHER
[rank7]:[E311 18:55:28.541093684 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0 Rank 7]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank1]:[E311 18:55:28.542919165 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200066 milliseconds before timing out.
[rank1]:[E311 18:55:28.544095765 ProcessGroupNCCL.cpp:1936] Rank 1
	 - [1] Timeout at collective: ALLGATHER, #1
	 - To our best knowledge, the lagging/dead/mismatched ranks that caused the desync are:
	   - [0, 1, 2, 3, 4, 5, 6, 7] joined but didn't finish collective #1 (count from 1)
	 - Snapshot of ranks' latest states:
	   #1 started ranks:
	     [0, 1, 2, 3, 4, 5, 6, 7] started ALLGATHER
[rank1]:[E311 18:55:28.544104807 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0 Rank 1]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank5]:[E311 18:55:28.560048188 ProcessGroupNCCL.cpp:629] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200057 milliseconds before timing out.
[rank5]:[E311 18:55:28.561399566 ProcessGroupNCCL.cpp:1936] Rank 5
	 - [5] Timeout at collective: ALLGATHER, #1
	 - To our best knowledge, the lagging/dead/mismatched ranks that caused the desync are:
	   - [0, 1, 2, 3, 4, 5, 6, 7] joined but didn't finish collective #1 (count from 1)
	 - Snapshot of ranks' latest states:
	   #1 started ranks:
	     [0, 1, 2, 3, 4, 5, 6, 7] started ALLGATHER
[rank5]:[E311 18:55:28.561411553 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0 Rank 5]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank3]:[E311 18:55:28.564297364 ProcessGroupNCCL.cpp:629] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200084 milliseconds before timing out.
[rank3]:[E311 18:55:28.565637208 ProcessGroupNCCL.cpp:1936] Rank 3
	 - [3] Timeout at collective: ALLGATHER, #1
	 - To our best knowledge, the lagging/dead/mismatched ranks that caused the desync are:
	   - [0, 1, 2, 3, 4, 5, 6, 7] joined but didn't finish collective #1 (count from 1)
	 - Snapshot of ranks' latest states:
	   #1 started ranks:
	     [0, 1, 2, 3, 4, 5, 6, 7] started ALLGATHER
[rank3]:[E311 18:55:28.565652869 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0 Rank 3]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank4]:[E311 18:55:28.568062802 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 _verify_param_shape_across_processes from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/distributed/utils.py:294
#1 __init__ from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:825
#2 train_model_distributed from /home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:579
#3 _wrap from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:90
#4 run from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:108
#5 _bootstrap from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:314
#6 _main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:135
#7 spawn_main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:122
#8 <module> from <string>:1

[rank0]:[E311 18:55:28.589146658 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 _verify_param_shape_across_processes from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/distributed/utils.py:294
#1 __init__ from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:825
#2 train_model_distributed from /home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:579
#3 _wrap from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:90
#4 run from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:108
#5 _bootstrap from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:314
#6 _main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:135
#7 spawn_main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:122
#8 <module> from <string>:1

[rank2]:[E311 18:55:28.595855445 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 _verify_param_shape_across_processes from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/distributed/utils.py:294
#1 __init__ from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:825
#2 train_model_distributed from /home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:579
#3 _wrap from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:90
#4 run from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:108
#5 _bootstrap from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:314
#6 _main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:135
#7 spawn_main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:122
#8 <module> from <string>:1

[rank1]:[E311 18:55:28.610114014 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 _verify_param_shape_across_processes from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/distributed/utils.py:294
#1 __init__ from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:825
#2 train_model_distributed from /home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:579
#3 _wrap from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:90
#4 run from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:108
#5 _bootstrap from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:314
#6 _main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:135
#7 spawn_main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:122
#8 <module> from <string>:1

[rank3]:[E311 18:55:28.616056968 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 _verify_param_shape_across_processes from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/distributed/utils.py:294
#1 __init__ from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:825
#2 train_model_distributed from /home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:579
#3 _wrap from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:90
#4 run from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:108
#5 _bootstrap from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:314
#6 _main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:135
#7 spawn_main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:122
#8 <module> from <string>:1

[rank7]:[E311 18:55:28.616418645 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 _verify_param_shape_across_processes from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/distributed/utils.py:294
#1 __init__ from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:825
#2 train_model_distributed from /home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:579
#3 _wrap from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:90
#4 run from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:108
#5 _bootstrap from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:314
#6 _main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:135
#7 spawn_main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:122
#8 <module> from <string>:1

[rank5]:[E311 18:55:28.634167905 ProcessGroupNCCL.cpp:664] Stack trace of the failed collective: 
#0 _verify_param_shape_across_processes from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/distributed/utils.py:294
#1 __init__ from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:825
#2 train_model_distributed from /home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:579
#3 _wrap from /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:90
#4 run from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:108
#5 _bootstrap from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/process.py:314
#6 _main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:135
#7 spawn_main from /home/dapgrad/tenzinl2/miniconda3/lib/python3.12/multiprocessing/spawn.py:122
#8 <module> from <string>:1

[rank0]:[E311 18:55:28.687596829 ProcessGroupNCCL.cpp:1753] [PG ID 0 PG GUID 0 Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank4]:[E311 18:55:28.687667495 ProcessGroupNCCL.cpp:1753] [PG ID 0 PG GUID 0 Rank 4] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank4]:[E311 18:55:28.702604102 ProcessGroupNCCL.cpp:1554] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank4]:[E311 18:55:28.703156134 ProcessGroupNCCL.cpp:681] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E311 18:55:28.703273900 ProcessGroupNCCL.cpp:695] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E311 18:55:28.704040664 ProcessGroupNCCL.cpp:1554] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E311 18:55:28.704376941 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E311 18:55:28.704385473 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E311 18:55:28.708691076 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200040 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe773c261b6 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fe774ea8e14 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fe774eaa970 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fe774eab88d in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fe7c80da5c0 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fe7c8d7fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fe7c8e11850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E311 18:55:28.710310821 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200028 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3b63c261b6 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f3b64ea8e14 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f3b64eaa970 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f3b64eab88d in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f3bb81465c0 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f3bb8debac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f3bb8e7d850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank2]:[E311 18:55:28.723876064 ProcessGroupNCCL.cpp:1753] [PG ID 0 PG GUID 0 Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank6]:[E311 18:55:28.741233901 ProcessGroupNCCL.cpp:1753] [PG ID 0 PG GUID 0 Rank 6] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank6]:[E311 18:55:28.749772054 ProcessGroupNCCL.cpp:1554] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank6]:[E311 18:55:28.750178279 ProcessGroupNCCL.cpp:681] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E311 18:55:28.750244598 ProcessGroupNCCL.cpp:695] [Rank 6] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E311 18:55:28.751844810 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200017 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6b3ed6c1b6 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f6aebea8e14 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f6aebeaa970 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f6aebeab88d in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f6b3f1825c0 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7f6b3fe27ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f6b3feb9850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank2]:[E311 18:55:28.752158360 ProcessGroupNCCL.cpp:1554] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank2]:[E311 18:55:28.752746814 ProcessGroupNCCL.cpp:681] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E311 18:55:28.752812072 ProcessGroupNCCL.cpp:695] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E311 18:55:28.754486432 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200066 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fabc04261b6 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fabc16a8e14 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fabc16aa970 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fabc16ab88d in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fac149365c0 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fac155dbac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fac1566d850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank7]:[E311 18:55:28.754660111 ProcessGroupNCCL.cpp:1753] [PG ID 0 PG GUID 0 Rank 7] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank7]:[E311 18:55:28.754979949 ProcessGroupNCCL.cpp:1554] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank7]:[E311 18:55:28.756401430 ProcessGroupNCCL.cpp:681] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E311 18:55:28.756470579 ProcessGroupNCCL.cpp:695] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E311 18:55:28.758052952 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200054 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff22036c1b6 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7ff1cd4a8e14 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7ff1cd4aa970 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ff1cd4ab88d in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ff2207b75c0 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ff22145cac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ff2214ee850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E311 18:55:28.758867648 ProcessGroupNCCL.cpp:1753] [PG ID 0 PG GUID 0 Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E311 18:55:28.758985813 ProcessGroupNCCL.cpp:1554] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank1]:[E311 18:55:28.759418822 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E311 18:55:28.759478200 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E311 18:55:28.761060201 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200066 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe6cb2261b6 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fe6cc4a8e14 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fe6cc4aa970 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fe6cc4ab88d in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fe71f6cf5c0 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fe720374ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fe720406850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[E311 18:55:28.788273547 ProcessGroupNCCL.cpp:1753] [PG ID 0 PG GUID 0 Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E311 18:55:28.788457902 ProcessGroupNCCL.cpp:1554] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank3]:[E311 18:55:28.788889277 ProcessGroupNCCL.cpp:681] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E311 18:55:28.788994641 ProcessGroupNCCL.cpp:695] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E311 18:55:28.790556247 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200084 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa931a261b6 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fa932ca8e14 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fa932caa970 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fa932cab88d in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fa985ebe5c0 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fa986b63ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fa986bf5850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank5]:[E311 18:55:28.795009274 ProcessGroupNCCL.cpp:1753] [PG ID 0 PG GUID 0 Rank 5] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank5]:[E311 18:55:28.795088570 ProcessGroupNCCL.cpp:1554] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank5]:[E311 18:55:28.795465966 ProcessGroupNCCL.cpp:681] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E311 18:55:28.795523946 ProcessGroupNCCL.cpp:695] [Rank 5] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E311 18:55:28.797063158 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLGATHER, NumelIn=1, NumelOut=8, Timeout(ms)=7200000) ran for 7200057 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fb81556c1b6 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7fb7c26a8e14 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7fb7c26aa970 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fb7c26ab88d in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fb815a175c0 in /home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7fb8166bcac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fb81674e850 in /lib/x86_64-linux-gnu/libc.so.6)

W0311 18:55:29.044000 4151 torch/multiprocessing/spawn.py:169] Terminating process 4160 via signal SIGTERM
W0311 18:55:29.046000 4151 torch/multiprocessing/spawn.py:169] Terminating process 4161 via signal SIGTERM
W0311 18:55:29.047000 4151 torch/multiprocessing/spawn.py:169] Terminating process 4162 via signal SIGTERM
W0311 18:55:29.048000 4151 torch/multiprocessing/spawn.py:169] Terminating process 4163 via signal SIGTERM
W0311 18:55:29.048000 4151 torch/multiprocessing/spawn.py:169] Terminating process 4165 via signal SIGTERM
W0311 18:55:29.049000 4151 torch/multiprocessing/spawn.py:169] Terminating process 4166 via signal SIGTERM
W0311 18:55:29.049000 4151 torch/multiprocessing/spawn.py:169] Terminating process 4167 via signal SIGTERM
Traceback (most recent call last):
  File "/home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py", line 851, in <module>
    main()
  File "/home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py", line 843, in main
    mp.spawn(
  File "/home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 340, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 296, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File "/home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 196, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 4 terminated with signal SIGABRT
srun: error: deepbull7: task 0: Exited with exit code 1
