2025-03-09 22:47:27,610 - INFO - Initialized wandb run: tablesense-20250309-224726
2025-03-09 22:47:27,611 - INFO - Process 0/4 starting training
2025-03-09 22:47:29,540 - INFO - Using provided sampler, shuffle set to False
2025-03-09 22:47:29,540 - INFO - Created DataLoader with: batch_size=1, num_workers=2, shuffle=False, sampler=provided
2025-03-09 22:48:11,575 - ERROR - Error in batch 1 on GPU 0: Detected mismatch between collectives on ranks. Rank 0 is running collective: CollectiveFingerPrint(SequenceNumber=11, OpType=ALLREDUCE, TensorShape=[659722], TensorDtypes=Float, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))), but Rank 2 is running collective: CollectiveFingerPrint(SequenceNumber=11, OpType=BROADCAST, TensorShape=[256], TensorDtypes=Float, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))).Collectives differ in the following aspects:   Op type: ALLREDUCEvs BROADCAST  Tensor Tensor shapes: 659722vs 256
2025-03-09 22:48:11,579 - ERROR - Error in batch 2 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,580 - ERROR - Error in batch 3 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,581 - ERROR - Error in batch 4 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,581 - ERROR - Error in batch 5 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,634 - ERROR - Error in batch 6 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,635 - ERROR - Error in batch 7 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,637 - ERROR - Error in batch 8 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,693 - ERROR - Error in batch 9 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,701 - ERROR - Error in batch 10 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,745 - ERROR - Error in batch 11 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,145 - ERROR - Error in batch 12 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,146 - ERROR - Error in batch 13 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,194 - ERROR - Error in batch 14 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,195 - ERROR - Error in batch 15 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,209 - ERROR - Error in batch 16 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,214 - ERROR - Error in batch 17 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,241 - ERROR - Error in batch 18 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,438 - ERROR - Error in batch 19 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,439 - ERROR - Error in batch 20 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,449 - ERROR - Error in batch 21 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,450 - ERROR - Error in batch 22 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,571 - ERROR - Error in batch 23 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,572 - ERROR - Error in batch 24 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,589 - ERROR - Error in batch 25 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,590 - ERROR - Error in batch 26 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,638 - ERROR - Error in batch 27 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,639 - ERROR - Error in batch 28 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
