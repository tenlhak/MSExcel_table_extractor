2025-03-09 17:03:19,569 - INFO - Process 0/4 starting training
2025-03-09 17:03:22,561 - INFO - Using provided sampler, shuffle set to False
2025-03-09 17:03:22,562 - INFO - Created DataLoader with: batch_size=1, num_workers=2, shuffle=False, sampler=provided
2025-03-09 19:32:10,888 - ERROR - Error in batch 5 on GPU 0: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 23.67 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 22.29 GiB memory in use. Of the allocated memory 21.73 GiB is allocated by PyTorch, and 79.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-09 19:32:12,590 - ERROR - Error in batch 6 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:12,617 - ERROR - Error in batch 7 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:12,650 - ERROR - Error in batch 8 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:12,651 - ERROR - Error in batch 9 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:12,912 - ERROR - Error in batch 10 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,011 - ERROR - Error in batch 11 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,057 - ERROR - Error in batch 12 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,061 - ERROR - Error in batch 13 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,079 - ERROR - Error in batch 14 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,129 - ERROR - Error in batch 15 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,147 - ERROR - Error in batch 16 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,336 - ERROR - Error in batch 17 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,337 - ERROR - Error in batch 18 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,351 - ERROR - Error in batch 19 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,352 - ERROR - Error in batch 20 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:13,381 - ERROR - Error in batch 21 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:14,662 - ERROR - Error in batch 22 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:30,712 - ERROR - Error in batch 23 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:30,713 - ERROR - Error in batch 24 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:30,722 - ERROR - Error in batch 25 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:30,722 - ERROR - Error in batch 26 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:30,733 - ERROR - Error in batch 27 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:31,464 - ERROR - Error in batch 28 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:31,467 - ERROR - Error in batch 29 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:31,584 - ERROR - Error in batch 30 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,125 - ERROR - Error in batch 31 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,126 - ERROR - Error in batch 32 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,169 - ERROR - Error in batch 33 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,170 - ERROR - Error in batch 34 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,195 - ERROR - Error in batch 35 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,196 - ERROR - Error in batch 36 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,292 - ERROR - Error in batch 37 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,328 - ERROR - Error in batch 38 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,329 - ERROR - Error in batch 39 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,341 - ERROR - Error in batch 40 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,435 - ERROR - Error in batch 41 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,436 - ERROR - Error in batch 42 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,466 - ERROR - Error in batch 43 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,467 - ERROR - Error in batch 44 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,591 - ERROR - Error in batch 45 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,592 - ERROR - Error in batch 46 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,605 - ERROR - Error in batch 47 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,606 - ERROR - Error in batch 48 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,731 - ERROR - Error in batch 49 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,732 - ERROR - Error in batch 50 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,758 - ERROR - Error in batch 51 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,915 - ERROR - Error in batch 52 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,916 - ERROR - Error in batch 53 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,925 - ERROR - Error in batch 54 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,926 - ERROR - Error in batch 55 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,959 - ERROR - Error in batch 56 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,959 - ERROR - Error in batch 57 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,970 - ERROR - Error in batch 58 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,971 - ERROR - Error in batch 59 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:33,979 - ERROR - Error in batch 60 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,291 - ERROR - Error in batch 61 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,293 - ERROR - Error in batch 62 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,436 - ERROR - Error in batch 63 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,437 - ERROR - Error in batch 64 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,482 - ERROR - Error in batch 65 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,485 - ERROR - Error in batch 66 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,491 - ERROR - Error in batch 67 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,492 - ERROR - Error in batch 68 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,551 - ERROR - Error in batch 69 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,899 - ERROR - Error in batch 70 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,901 - ERROR - Error in batch 71 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,922 - ERROR - Error in batch 72 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:34,923 - ERROR - Error in batch 73 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:38,034 - ERROR - Error in batch 74 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:38,035 - ERROR - Error in batch 75 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:38,073 - ERROR - Error in batch 76 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:38,074 - ERROR - Error in batch 77 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:38,179 - ERROR - Error in batch 78 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:38,192 - ERROR - Error in batch 79 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:38,196 - ERROR - Error in batch 80 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,032 - ERROR - Error in batch 81 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,034 - ERROR - Error in batch 82 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,063 - ERROR - Error in batch 83 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,064 - ERROR - Error in batch 84 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,111 - ERROR - Error in batch 85 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,112 - ERROR - Error in batch 86 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,140 - ERROR - Error in batch 87 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,744 - ERROR - Error in batch 88 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,745 - ERROR - Error in batch 89 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,906 - ERROR - Error in batch 90 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,907 - ERROR - Error in batch 91 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,994 - ERROR - Error in batch 92 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:41,996 - ERROR - Error in batch 93 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,046 - ERROR - Error in batch 94 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,047 - ERROR - Error in batch 95 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,067 - ERROR - Error in batch 96 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,068 - ERROR - Error in batch 97 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,143 - ERROR - Error in batch 98 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,145 - ERROR - Error in batch 99 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,162 - ERROR - Error in batch 100 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,163 - ERROR - Error in batch 101 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,208 - ERROR - Error in batch 102 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,579 - ERROR - Error in batch 103 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,580 - ERROR - Error in batch 104 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,692 - ERROR - Error in batch 105 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,693 - ERROR - Error in batch 106 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,714 - ERROR - Error in batch 107 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,714 - ERROR - Error in batch 108 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,738 - ERROR - Error in batch 109 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:42,739 - ERROR - Error in batch 110 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,078 - ERROR - Error in batch 111 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,079 - ERROR - Error in batch 112 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,091 - ERROR - Error in batch 113 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,092 - ERROR - Error in batch 114 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,103 - ERROR - Error in batch 115 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,496 - ERROR - Error in batch 116 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,497 - ERROR - Error in batch 117 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,524 - ERROR - Error in batch 118 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,524 - ERROR - Error in batch 119 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
2025-03-09 19:32:43,529 - ERROR - Error in process 0: Detected mismatch between collectives on ranks. Rank 0 is running collective: CollectiveFingerPrint(SequenceNumber=27OpType=BARRIER), but Rank 1 is running collective: CollectiveFingerPrint(SequenceNumber=0OpType=GATHER).Collectives differ in the following aspects: 	 Sequence number: 27vs 0  Op type: BARRIERvs GATHER
