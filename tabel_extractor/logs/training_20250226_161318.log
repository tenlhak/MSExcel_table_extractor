2025-02-26 16:13:18,544 - INFO - Process 0/4 starting training
2025-02-26 16:13:21,495 - INFO - Using provided sampler, shuffle set to False
2025-02-26 16:13:21,496 - INFO - Created DataLoader with: batch_size=1, num_workers=2, shuffle=False, sampler=provided
2025-02-26 16:25:58,390 - ERROR - Error in batch 2 on GPU 0: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 140.69 MiB is free. Including non-PyTorch memory, this process has 10.61 GiB memory in use. Of the allocated memory 10.14 GiB is allocated by PyTorch, and 195.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-02-26 16:25:59,847 - ERROR - Error in batch 3 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:25:59,881 - ERROR - Error in batch 4 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:25:59,911 - ERROR - Error in batch 5 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:25:59,939 - ERROR - Error in batch 6 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:25:59,958 - ERROR - Error in batch 7 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:25:59,959 - ERROR - Error in batch 8 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:25:59,960 - ERROR - Error in batch 9 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,076 - ERROR - Error in batch 10 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,148 - ERROR - Error in batch 11 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,229 - ERROR - Error in batch 12 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,231 - ERROR - Error in batch 13 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,235 - ERROR - Error in batch 14 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,237 - ERROR - Error in batch 15 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,370 - ERROR - Error in batch 16 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,418 - ERROR - Error in batch 17 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,419 - ERROR - Error in batch 18 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,431 - ERROR - Error in batch 19 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,432 - ERROR - Error in batch 20 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:00,587 - ERROR - Error in batch 21 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-26 16:26:01,128 - ERROR - Error in batch 22 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
