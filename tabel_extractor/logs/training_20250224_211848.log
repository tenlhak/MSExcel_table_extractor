2025-02-24 21:18:48,689 - INFO - Process 0/4 starting training
2025-02-24 21:18:51,666 - INFO - Using provided sampler, shuffle set to False
2025-02-24 21:18:51,666 - INFO - Created DataLoader with: batch_size=1, num_workers=2, shuffle=False, sampler=provided
2025-02-24 21:27:18,633 - INFO - GPU 0: Gradient norm before clipping: 4.223087310791016 in batch 0
2025-02-24 21:29:35,021 - INFO - GPU 0: Gradient norm before clipping: 5.630678653717041 in batch 1
2025-02-24 21:31:37,214 - ERROR - Error in batch 2 on GPU 0: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 92.69 MiB is free. Including non-PyTorch memory, this process has 10.65 GiB memory in use. Of the allocated memory 10.14 GiB is allocated by PyTorch, and 243.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-02-24 21:31:38,700 - ERROR - Error in batch 3 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:38,734 - ERROR - Error in batch 4 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:38,764 - ERROR - Error in batch 5 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:38,792 - ERROR - Error in batch 6 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:38,811 - ERROR - Error in batch 7 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:38,811 - ERROR - Error in batch 8 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:38,812 - ERROR - Error in batch 9 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:38,955 - ERROR - Error in batch 10 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,070 - ERROR - Error in batch 11 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,103 - ERROR - Error in batch 12 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,105 - ERROR - Error in batch 13 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,151 - ERROR - Error in batch 14 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,153 - ERROR - Error in batch 15 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,160 - ERROR - Error in batch 16 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,314 - ERROR - Error in batch 17 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,316 - ERROR - Error in batch 18 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,323 - ERROR - Error in batch 19 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,324 - ERROR - Error in batch 20 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:39,331 - ERROR - Error in batch 21 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:40,136 - ERROR - Error in batch 22 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:55,037 - ERROR - Error in batch 23 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:55,039 - ERROR - Error in batch 24 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:55,050 - ERROR - Error in batch 25 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:55,052 - ERROR - Error in batch 26 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:55,113 - ERROR - Error in batch 27 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:55,611 - ERROR - Error in batch 28 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:55,612 - ERROR - Error in batch 29 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:55,620 - ERROR - Error in batch 30 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,838 - ERROR - Error in batch 31 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,840 - ERROR - Error in batch 32 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,848 - ERROR - Error in batch 33 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,850 - ERROR - Error in batch 34 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,894 - ERROR - Error in batch 35 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,919 - ERROR - Error in batch 36 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,922 - ERROR - Error in batch 37 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,975 - ERROR - Error in batch 38 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,977 - ERROR - Error in batch 39 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:58,987 - ERROR - Error in batch 40 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:59,011 - ERROR - Error in batch 41 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:59,085 - ERROR - Error in batch 42 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
2025-02-24 21:31:59,086 - ERROR - Error in batch 43 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.pbr_head.0.bias, detection_head.pbr_head.0.weight, detection_head.bbr_head.bias, detection_head.bbr_head.weight, detection_head.classifier.bias, detection_head.classifier.weight, detection_head.shared_network.4.bias, detection_head.shared_network.4.weight, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
