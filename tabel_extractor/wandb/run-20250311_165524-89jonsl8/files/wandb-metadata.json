{
  "os":  "Linux-5.15.0-133-generic-x86_64-with-glibc2.35",
  "python":  "CPython 3.12.3",
  "startedAt":  "2025-03-11T20:55:24.802477Z",
  "args":  [
    "--annotation-file",
    "/home/dapgrad/tenzinl2/lumina/luminaData/processed_enron_paths_filtered.csv",
    "--profile",
    "--profile-batches",
    "10",
    "--use-wandb",
    "--wandb-project",
    "tablesense",
    "--wandb-entity",
    "tenlhak98-university-at-buffalo",
    "--wandb-tags",
    "baseline,high-res"
  ],
  "program":  "/home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py",
  "codePath":  "scripts/train.py",
  "email":  "tenlhak98@gmail.com",
  "root":  "/home/dapgrad/tenzinl2/lumina/tabel_extractor",
  "host":  "deepbull7",
  "executable":  "/home/dapgrad/tenzinl2/lumina/lumina/bin/python",
  "codePathLocal":  "scripts/train.py",
  "cpu_count":  40,
  "cpu_count_logical":  80,
  "gpu":  "NVIDIA RTX A6000",
  "gpu_count":  8,
  "disk":  {
    "/":  {
      "total":  "105089261568",
      "used":  "9055649792"
    }
  },
  "memory":  {
    "total":  "540656861184"
  },
  "cpu":  {
    "count":  40,
    "countLogical":  80
  },
  "gpu_nvidia":  [
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    }
  ],
  "slurm":  {
    "cluster_name":  "a2il",
    "conf":  "/etc/slurm/slurm.conf",
    "cpu_bind":  "quiet,mask_cpu:0x00001000010000100001",
    "cpu_bind_list":  "0x00001000010000100001",
    "cpu_bind_type":  "mask_cpu:",
    "cpu_bind_verbose":  "quiet",
    "cpus_on_node":  "4",
    "cpus_per_task":  "4",
    "distribution":  "cyclic",
    "gpus_on_node":  "8",
    "gtids":  "0",
    "job_account":  "a2il",
    "job_cpus_per_node":  "4",
    "job_end_time":  "1741985703",
    "job_gid":  "330790",
    "job_gpus":  "0,1,2,3,4,5,6,7",
    "job_id":  "1100",
    "job_name":  "training",
    "job_nodelist":  "deepbull7",
    "job_num_nodes":  "1",
    "job_partition":  "a2il",
    "job_qos":  "normal",
    "job_start_time":  "1741726503",
    "job_uid":  "815837",
    "job_user":  "tenzinl2",
    "jobid":  "1100",
    "launch_node_ipaddr":  "127.0.0.1",
    "localid":  "0",
    "mem_per_node":  "32768",
    "nnodes":  "1",
    "nodeid":  "0",
    "nodelist":  "deepbull7",
    "nprocs":  "1",
    "ntasks":  "1",
    "prio_process":  "0",
    "procid":  "0",
    "srun_comm_host":  "127.0.0.1",
    "srun_comm_port":  "36541",
    "step_gpus":  "0,1,2,3,4,5,6,7",
    "step_id":  "0",
    "step_launcher_port":  "36541",
    "step_nodelist":  "deepbull7",
    "step_num_nodes":  "1",
    "step_num_tasks":  "1",
    "step_tasks_per_node":  "1",
    "stepid":  "0",
    "submit_dir":  "/home/dapgrad/tenzinl2/lumina/tabel_extractor",
    "submit_host":  "deepbull-usr1",
    "task_pid":  "4151",
    "tasks_per_node":  "1",
    "topology_addr":  "deepbull7",
    "topology_addr_pattern":  "node",
    "tres_per_task":  "cpu=4",
    "umask":  "0022"
  },
  "cudaVersion":  "12.7"
}