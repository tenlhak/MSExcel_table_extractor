2025-03-09 22:47:27,610 - INFO - Initialized wandb run: tablesense-20250309-224726
2025-03-09 22:47:27,611 - INFO - Process 0/4 starting training
[34m[1mwandb[0m: logging graph, to disable use `wandb.watch(log_graph=False)`
2025-03-09 22:47:29,540 - INFO - Using provided sampler, shuffle set to False
2025-03-09 22:47:29,540 - INFO - Created DataLoader with: batch_size=1, num_workers=2, shuffle=False, sampler=provided
/home/dapgrad/tenzinl2/lumina/tabel_extractor/scripts/train.py:613: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler() if args.mixed_precision else None
/home/dapgrad/tenzinl2/lumina/lumina/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Epoch 0:   0%|                                                                                                                                                          | 0/92 [00:00<?, ?it/s]/home/dapgrad/tenzinl2/lumina/tabel_extractor/tablesense/utils/training_utils.py:359: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
The max anchor overlap is 0.7347400188446045

RPN Labels statistics:
Total proposals: 19800
Positive anchors proposals: 18
Negative anchors proposals: 19782
The max ROI overlap is 0.783000648021698

ROI Labels statistics:
Total ROI proposals: 19800
Positive ROI proposals: 16
Negative ROI proposals: 19784

*******Following are the LOSSes***********
The rpn class loss is 0.7055484056472778
The rpn bbox loss is 0.021955275908112526
The det_class_loss is 0.0005826823180541396
The detection head bbox loss is 0.011534209363162518
The detection head precise bbox loss is 0.004006687551736832
Epoch 0:   1%|█                                                                                                | 1/92 [00:27<41:23, 27.30s/it, gpu=0, loss=0.7436, avg_loss=0.7436, mem=3.16GB]2025-03-09 22:48:11,575 - ERROR - Error in batch 1 on GPU 0: Detected mismatch between collectives on ranks. Rank 0 is running collective: CollectiveFingerPrint(SequenceNumber=11, OpType=ALLREDUCE, TensorShape=[659722], TensorDtypes=Float, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))), but Rank 2 is running collective: CollectiveFingerPrint(SequenceNumber=11, OpType=BROADCAST, TensorShape=[256], TensorDtypes=Float, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))).Collectives differ in the following aspects:   Op type: ALLREDUCEvs BROADCAST  Tensor Tensor shapes: 659722vs 256
The max anchor overlap is 0.725978672504425

RPN Labels statistics:
Total proposals: 23004
Positive anchors proposals: 27
Negative anchors proposals: 22977
The max ROI overlap is 0.9567238688468933

ROI Labels statistics:
Total ROI proposals: 23004
Positive ROI proposals: 48
Negative ROI proposals: 22956

*******Following are the LOSSes***********
The rpn class loss is 0.6568704843521118
The rpn bbox loss is 0.032376233488321304
The det_class_loss is 0.0012011858634650707
The detection head bbox loss is 0.5135773420333862
The detection head precise bbox loss is 0.018963659182190895
Epoch 0:   2%|██                                                                                               | 2/92 [00:42<29:51, 19.91s/it, gpu=0, loss=0.7436, avg_loss=0.7436, mem=3.16GB]2025-03-09 22:48:11,579 - ERROR - Error in batch 2 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,580 - ERROR - Error in batch 3 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,581 - ERROR - Error in batch 4 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,581 - ERROR - Error in batch 5 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,634 - ERROR - Error in batch 6 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,635 - ERROR - Error in batch 7 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,637 - ERROR - Error in batch 8 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,693 - ERROR - Error in batch 9 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
Epoch 0:  11%|██████████▍                                                                                     | 10/92 [00:42<03:32,  2.59s/it, gpu=0, loss=0.7436, avg_loss=0.7436, mem=3.16GB]2025-03-09 22:48:11,701 - ERROR - Error in batch 10 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:11,745 - ERROR - Error in batch 11 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,145 - ERROR - Error in batch 12 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,146 - ERROR - Error in batch 13 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,194 - ERROR - Error in batch 14 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,195 - ERROR - Error in batch 15 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,209 - ERROR - Error in batch 16 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
Epoch 0:  18%|█████████████████▋                                                                              | 17/92 [00:42<01:35,  1.28s/it, gpu=0, loss=0.7436, avg_loss=0.7436, mem=3.16GB]2025-03-09 22:48:12,214 - ERROR - Error in batch 17 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,241 - ERROR - Error in batch 18 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,438 - ERROR - Error in batch 19 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,439 - ERROR - Error in batch 20 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
Epoch 0:  23%|█████████████████████▉                                                                          | 21/92 [00:42<01:04,  1.09it/s, gpu=0, loss=0.7436, avg_loss=0.7436, mem=3.16GB]2025-03-09 22:48:12,449 - ERROR - Error in batch 21 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,450 - ERROR - Error in batch 22 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,571 - ERROR - Error in batch 23 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
Epoch 0:  26%|█████████████████████████                                                                       | 24/92 [00:43<00:47,  1.42it/s, gpu=0, loss=0.7436, avg_loss=0.7436, mem=3.16GB]2025-03-09 22:48:12,572 - ERROR - Error in batch 24 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,589 - ERROR - Error in batch 25 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,590 - ERROR - Error in batch 26 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,638 - ERROR - Error in batch 27 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
2025-03-09 22:48:12,639 - ERROR - Error in batch 28 on GPU 0: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: detection_head.shared_network.4.bias, detection_head.shared_network.1.bias, detection_head.shared_network.1.weight, rpn.regression_head.bias, rpn.regression_head.weight, rpn.classification_head.bias, rpn.classification_head.weight, rpn.feature_extractor.0.bias, rpn.feature_extractor.0.weight, feature_processor.4.bias, feature_processor.4.weight, feature_processor.3.bias, feature_processor.3.weight, feature_processor.1.bias, feature_processor.1.weight, feature_processor.0.bias, feature_processor.0.weight
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17
Epoch 0:  32%|██████████████████████████████▎                                                                 | 29/92 [00:43<01:33,  1.49s/it, gpu=0, loss=0.7436, avg_loss=0.7436, mem=3.16GB]
